# ğŸ•·ï¸ IntegraÃ§Ã£o do Scraper Python

Este documento descreve como o scraper Python foi integrado ao backend Node.js do BriefFlow.

## ğŸ¯ Objetivo

Permitir que o frontend e o backend Node.js acionem o scraper Python para coletar conteÃºdo automaticamente.

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Backend Node.js â”‚
â”‚  (React/Vite)   â”‚                  â”‚   (Porta 5000)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚ HTTP
                                              â–¼
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚  Scraper Python  â”‚
                                     â”‚  FastAPI:8000    â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚      SQLite      â”‚
                                     â”‚  (briefflow.db)  â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Arquivos Criados/Modificados

### 1. ServiÃ§o de IntegraÃ§Ã£o
**Arquivo:** `server/services/scraper.ts`

ServiÃ§o TypeScript que faz chamadas HTTP ao scraper Python:
- `checkScraperHealth()` - Verifica se o scraper estÃ¡ disponÃ­vel
- `startScraping()` - Inicia uma tarefa de scraping
- `getTaskStatus()` - ObtÃ©m status de uma tarefa
- `scrapeUrl()` - Faz scraping de URL especÃ­fica
- `testSource()` - Testa uma fonte antes de adicionar

### 2. Rotas da API
**Arquivo:** `server/routes.ts`

Novas rotas adicionadas ao backend Node.js:

| Rota | MÃ©todo | DescriÃ§Ã£o |
|------|--------|-----------|
| `/api/scraper/health` | GET | Verifica saÃºde do scraper |
| `/api/clients/:clientId/scrape` | POST | Inicia scraping para um cliente |
| `/api/scraper/tasks/:taskId` | GET | Status de uma tarefa |
| `/api/scraper/tasks` | GET | Lista todas as tarefas |
| `/api/scraper/scrape-url` | POST | Scraping de URL especÃ­fica |
| `/api/scraper/test-source` | POST | Testa uma fonte |
| `/api/clients/:clientId/sync-contents` | POST | Sincroniza conteÃºdos |

### 3. Scripts de InicializaÃ§Ã£o

**Linux/Mac:** `start-briefflow.sh`
**Windows:** `start-briefflow.ps1`

Iniciam ambos os serviÃ§os (Node.js + Python) automaticamente.

### 4. ConfiguraÃ§Ã£o
**Arquivo:** `.env`

```env
# URL do serviÃ§o de scraper Python
SCRAPER_API_URL=http://localhost:8000
```

## ğŸš€ Como Usar

### 1. Iniciar Tudo de Uma Vez

**Linux/Mac:**
```bash
chmod +x start-briefflow.sh
./start-briefflow.sh
```

**Windows:**
```powershell
.\start-briefflow.ps1
```

Isso inicia:
- Scraper Python na porta 8000
- Backend Node.js na porta 5000

### 2. Iniciar Manualmente (Desenvolvimento)

Terminal 1 - Scraper:
```bash
cd scraper
source venv/bin/activate  # Windows: .\venv\Scripts\activate
python src/api/server.py
```

Terminal 2 - Backend:
```bash
npm run dev
```

### 3. Fazer Scraping de um Cliente

```bash
# Iniciar scraping para um cliente especÃ­fico
curl -X POST http://localhost:5000/api/clients/{client_id}/scrape

# Com fontes especÃ­ficas
curl -X POST http://localhost:5000/api/clients/{client_id}/scrape \
  -H "Content-Type: application/json" \
  -d '{"source_ids": ["source-id-1", "source-id-2"]}'

# ForÃ§ar re-scraping
curl -X POST http://localhost:5000/api/clients/{client_id}/scrape \
  -H "Content-Type: application/json" \
  -d '{"force_rescrape": true}'
```

### 4. Verificar Status

```bash
# Status do scraper
curl http://localhost:5000/api/scraper/health

# Status de uma tarefa
curl http://localhost:5000/api/scraper/tasks/{task_id}

# Todas as tarefas
curl http://localhost:5000/api/scraper/tasks
```

### 5. Testar uma Fonte

```bash
curl -X POST http://localhost:5000/api/scraper/test-source \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://exemplo.com/feed.xml",
    "type": "rss"
  }'
```

## ğŸ”Œ Fluxo de Dados

### Scraping AutomÃ¡tico
1. UsuÃ¡rio chama `POST /api/clients/{id}/scrape`
2. Backend Node.js valida o cliente
3. Backend busca fontes do cliente (ou usa as fornecidas)
4. Backend chama `POST http://localhost:8000/scrape`
5. Scraper Python processa as fontes
6. Scraper salva conteÃºdo no SQLite
7. Backend retorna `task_id` para acompanhamento

### SincronizaÃ§Ã£o de ConteÃºdo
1. UsuÃ¡rio chama `POST /api/clients/{id}/sync-contents`
2. Backend chama `GET http://localhost:8000/clients/{id}/contents`
3. Backend recebe conteÃºdos do scraper
4. (Opcional) Backend sincroniza com banco local

## ğŸ› ï¸ Desenvolvimento

### Adicionar Nova Funcionalidade

1. Adicione a funÃ§Ã£o em `server/services/scraper.ts`:
```typescript
export async function novaFuncao(param: string): Promise<Retorno> {
  const response = await fetch(`${SCRAPER_API_URL}/endpoint`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ param }),
  });
  return response.json();
}
```

2. Adicione a rota em `server/routes.ts`:
```typescript
app.post("/api/scraper/nova-rota", async (req, res) => {
  try {
    const result = await novaFuncao(req.body.param);
    res.json(result);
  } catch (error: any) {
    res.status(500).json({ message: error.message });
  }
});
```

## ğŸ”§ Troubleshooting

### Scraper nÃ£o responde
```bash
# Verificar se estÃ¡ rodando
curl http://localhost:8000/health

# Ver logs do scraper
cd scraper
python src/api/server.py
```

### Erro de CORS
O scraper jÃ¡ estÃ¡ configurado para aceitar requisiÃ§Ãµes de qualquer origem em desenvolvimento. Em produÃ§Ã£o, configure `allow_origins` em `scraper/src/api/server.py`.

### Banco de dados nÃ£o encontrado
Ambos os serviÃ§os usam o mesmo banco SQLite (`data/briefflow.db`). Verifique se o arquivo existe:
```bash
ls -la data/briefflow.db
```

## ğŸ“‹ PrÃ³ximos Passos

- [ ] Implementar agendamento automÃ¡tico (cron)
- [ ] Adicionar webhook para notificaÃ§Ãµes de novos conteÃºdos
- [ ] Criar fila de processamento para grandes volumes
- [ ] Implementar retry automÃ¡tico em caso de falha

---

**Nota:** Esta integraÃ§Ã£o mantÃ©m o scraper Python como serviÃ§o separado, permitindo migraÃ§Ã£o futura para arquitetura 100% Python se necessÃ¡rio.
