# BriefFlow Content Scraper

Sistema de coleta de conteÃºdo de mÃºltiplas fontes (RSS, blogs, sites de notÃ­cias) desenvolvido em Python.

## ğŸš€ Funcionalidades

- **RSS/Atom Feeds**: ExtraÃ§Ã£o automÃ¡tica de feeds RSS e Atom
- **Web Scraping**: Coleta de conteÃºdo de sites genÃ©ricos com detecÃ§Ã£o inteligente de estrutura
- **Processamento de Texto**: ExtraÃ§Ã£o de entidades, resumos e anÃ¡lise de sentimentos
- **API REST**: Interface completa para integraÃ§Ã£o com o backend Node.js
- **Agendamento**: Sistema de agendamento de tarefas de scraping
- **Monitoramento**: Status em tempo real das tarefas de scraping

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- SQLite 3
- Node.js 18+ (backend)

## ğŸ”§ InstalaÃ§Ã£o

### Windows

```bash
# Executar script de instalaÃ§Ã£o
setup.bat

# Ou manualmente:
python -m venv venv
venv\Scripts\activate.bat
pip install -r requirements.txt
python -m spacy download en_core_web_sm
python -m spacy download pt_core_news_sm
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('vader_lexicon')"
```

### Linux/macOS

```bash
# Executar script de instalaÃ§Ã£o
chmod +x setup.sh
./setup.sh

# Ou manualmente:
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python -m spacy download en_core_web_sm
python -m spacy download pt_core_news_sm
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('vader_lexicon')"
```

## âš™ï¸ ConfiguraÃ§Ã£o

1. Copie `.env.example` para `.env`
2. Configure as variÃ¡veis de ambiente:

```env
# Ambiente
ENVIRONMENT=development

# API do Scraper
SCRAPER_API_HOST=127.0.0.1
SCRAPER_API_PORT=8000

# Banco de Dados
DATABASE_PATH=../data/briefflow.db

# API do BriefFlow
BRIEFFLOW_API_URL=http://localhost:5001
```

## ğŸš€ ExecuÃ§Ã£o

```bash
# Ativar ambiente virtual
# Windows: venv\Scripts\activate.bat
# Linux/macOS: source venv/bin/activate

# Iniciar API do scraper
python main.py
```

A API estarÃ¡ disponÃ­vel em: http://localhost:8000

## ğŸ“– DocumentaÃ§Ã£o da API

### Endpoints Principais

- `GET /` - Health check
- `GET /health` - Health check detalhado
- `GET /clients` - Listar clientes
- `GET /clients/{client_id}/sources` - Fontes de um cliente
- `POST /scrape` - Iniciar tarefa de scraping
- `GET /tasks/{task_id}` - Status da tarefa
- `POST /scrape-url` - Scraping de URL especÃ­fica
- `POST /test-source` - Testar fonte

### Exemplos de Uso

#### Iniciar Scraping

```bash
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{
    "client_ids": ["client-1"],
    "force_rescrape": false
  }'
```

#### Verificar Status da Tarefa

```bash
curl "http://localhost:8000/tasks/task-id-here"
```

#### Fazer Scraping de URL EspecÃ­fica

```bash
curl -X POST "http://localhost:8000/scrape-url" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/article"}'
```

#### Testar Fonte

```bash
curl -X POST "http://localhost:8000/test-source" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com/feed.xml",
    "source_type": "rss"
  }'
```

## ğŸ—ï¸ Estrutura

```
scraper/
â”œâ”€â”€ main.py                 # Ponto de entrada
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ .env                  # ConfiguraÃ§Ã£o
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ server.py     # API FastAPI
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ scraper.py   # Modelos de dados
â”‚   â”‚   â”œâ”€â”€ database.py  # Interface SQLite
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ rss_scraper.py    # Scraper RSS
â”‚   â”‚   â”œâ”€â”€ web_scraper.py    # Scraper Web
â”‚   â”‚   â”œâ”€â”€ scraper_manager.py # Gerenciador
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py     # ConfiguraÃ§Ã£o
â”‚       â”œâ”€â”€ logger.py     # Logging
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ data/                # DiretÃ³rio de dados
â”œâ”€â”€ logs/                # Logs
â””â”€â”€ venv/               # Ambiente virtual
```

## ğŸ” Tipos de Fontes Suportados

1. **RSS/Atom Feeds**
   - ExtraÃ§Ã£o automÃ¡tica de tÃ­tulo, conteÃºdo, autor e data
   - Suporte para namespaces populares
   - DetecÃ§Ã£o de conteÃºdo duplicado

2. **Blogs e Sites de NotÃ­cias**
   - DetecÃ§Ã£o automÃ¡tica de estrutura do site
   - Suporte para WordPress, Medium e plataformas populares
   - ExtraÃ§Ã£o de tÃ­tulo, conteÃºdo, autor, data e tags

3. **YouTube** (em desenvolvimento)
   - ExtraÃ§Ã£o de metadados de vÃ­deos
   - TranscriÃ§Ã£o de conteÃºdo

## ğŸ“Š Fluxo de Processamento

1. **Descoberta**: Identificar URLs de artigos
2. **ExtraÃ§Ã£o**: Baixar e parsear conteÃºdo
3. **Limpeza**: Remover HTML e formataÃ§Ã£o
4. **ValidaÃ§Ã£o**: Verificar qualidade do conteÃºdo
5. **Armazenamento**: Salvar no banco SQLite
6. **AnÃ¡lise**: Processamento com NLP (opcional)

## ğŸ› ï¸ Desenvolvimento

### Executar Testes

```bash
python -m pytest tests/
```

### Formatar CÃ³digo

```bash
black src/
flake8 src/
```

### Logs

Os logs sÃ£o salvos em `logs/scraper.log` e tambÃ©m exibidos no console.

## ğŸ¤ IntegraÃ§Ã£o com Backend Node.js

O scraper se integra com o backend atravÃ©s:

1. **Banco de dados SQLite compartilhado**
2. **API REST para comunicaÃ§Ã£o**
3. **Webhooks para notificaÃ§Ãµes** (em desenvolvimento)

## ğŸ”’ SeguranÃ§a

- Rate limiting entre requisiÃ§Ãµes
- ValidaÃ§Ã£o de conteÃºdo
- User-Agent personalizado
- Tratamento de erros robusto

## ğŸ“ˆ Monitoramento

- Health checks detalhados
- MÃ©tricas de scraping
- Status de tarefas em tempo real
- Logs estruturados

## ğŸš€ PrÃ³ximos Passos

- [ ] Implementar scraper do YouTube
- [ ] Adicionar processamento de NLP
- [ ] Implementar agendamento automÃ¡tico
- [ ] Adicionar cache de conteÃºdo
- [ ] Implementar webhooks

## ğŸ“ LicenÃ§a

MIT License - Veja o arquivo LICENSE para detalhes.